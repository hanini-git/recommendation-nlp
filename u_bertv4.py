# -*- coding: utf-8 -*-
"""U_BERTv4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sv4jT_rzKpk5uMLBodi7I5ed9ZUEWETn
"""

import warnings;warnings.filterwarnings('ignore')

from google.colab import drive

drive.mount("/content/gdrive/")

!pip install transformers

from transformers import BertTokenizer, BertModel, BertConfig

from torch.utils.data import Dataset,DataLoader

from tqdm import tqdm_notebook,tnrange

import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

!pip install bidict

from bidict import bidict
import math

import pandas as pd

import joblib

input_file_path = "/content/gdrive/My Drive/Colab Notebooks/U BERT/data.csv"

from pathlib import Path

"""### Split Data into Train and Test"""

data = pd.read_csv(input_file_path)

train = pd.DataFrame()
test = pd.DataFrame()
for grp,df in data.groupby(['reviewerID']):
    df = df.sample(frac=1)
    t = int(len(df)*0.8)
    train = train.append(df.iloc[:t])
    test = test.append(df.iloc[t:])
train  = train.dropna().reset_index(drop=True)
test  = test.dropna().reset_index(drop=True)

len(train),len(test)

train_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/train.csv"

train.to_csv(train_fpath,index=False)

test_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/test.csv"

test.to_csv(test_fpath,index=False)

"""### Create user to index mappings"""

user2idx={}
for user in data.reviewerID.unique():
    user2idx[user]=len(user2idx)
len(user2idx)

user2idx = bidict(user2idx)

user2idx_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/user2idx.pkl"

joblib.dump(user2idx,user2idx_fpath)

user2idx= joblib.load(user2idx_fpath)

"""### Default configurations of BERT"""

config = BertConfig()

config

"""### Load Pretrained bert model"""

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

tokenizer.sep_token, tokenizer.sep_token_id

tokenizer.cls_token, tokenizer.cls_token_id

tokenizer.pad_token, tokenizer.pad_token_id

tokenizer.unk_token, tokenizer.unk_token_id

train.head(n=2)

train = train[['reviewerID','reviewText','overall']]

test = test[['reviewerID','reviewText','overall']]

train.iloc[0]

train.iloc[0].reviewerID,train.iloc[0].reviewText,train.iloc[0].overall

"""### Dataset class for pretraining
- Returns input_ids, attention mask, user_ids and user rating for training
"""

class PreTrainingDataset(Dataset):
    def __init__(self,reviews,tokenizer,user2idx,max_len):
        self.reviews = reviews
        self.max_len = max_len
        self.tokenizer = tokenizer
        self.user2idx = user2idx
    
    def __len__(self): return len(self.reviews)
    
    def __getitem__(self,item):
        
        user_id = self.reviews[item][0]
        review_text = self.reviews[item][1]
        score = self.reviews[item][2]
        
        encoding = self.tokenizer.encode_plus(review_text,
                                              add_special_tokens=True,
                                              max_length=self.max_len,
                                              return_token_type_ids=False,
                                              pad_to_max_length=True,
                                              return_attention_mask=True,
                                              return_tensors='pt',
                                              truncation=True)
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'user' : torch.LongTensor([self.user2idx[user_id]]),
            'targets': torch.Tensor([score])
        }

"""### Maximum sequence length"""

max_len = 128

ds = PreTrainingDataset(train.to_numpy(),
                  tokenizer,
                  user2idx,
                  max_len
                  )

val_ds = PreTrainingDataset(test.to_numpy(),
                           tokenizer,
                           user2idx,
                           max_len)

"""### Dataloader creation"""

dl = DataLoader(ds,batch_size=4,shuffle=True)

val_dl = DataLoader(val_ds,batch_size=4)

"""### Load pretrained model for reviews encoder"""

model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

for data in dl:
    break

data

input_ids = data['input_ids']
attention_mask = data['attention_mask']
user_ids = data['user']
targets = data['targets']

seq_output,pooled_output = model(input_ids=input_ids,attention_mask=attention_mask)

seq_output.shape, pooled_output.shape

"""### DotProduct Attention
- mentioned as Attn in U-BERT paper
"""

class ScaledDotProductAttention(nn.Module):
    def __init__(self,dk):
        super().__init__()
        self.scale = dk**0.5
        
    def forward(self,Q,K,V,att_mask):
        scores = torch.matmul(Q,K.transpose(-2,-1))/self.scale
        
        if att_mask is not None:
            scores.masked_fill_(att_mask,-1e9)
            
        att = torch.softmax(scores,dim=-1)
        context = torch.matmul(att,V)
        return context,att

"""### GELU activation function"""

def gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))

"""### Feed Forward network
- mention as FFN in paper 
"""

class PositionwiseFeedForward(nn.Module):
    def __init__(self,d_model,d_ff):
        super().__init__()
        self.ff1 = nn.Linear(d_model,d_ff)
        self.act = gelu
        self.ff2 = nn.Linear(d_ff,d_model)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self,x):
        residual = x
        
        x = self.ff1(x)
        x = self.act(x)
        x = self.ff2(x)
        x = self.layer_norm(x + residual)
        return x

# UI - User (or) Item

"""### User / Item emebdding
- class named as *UIEmbedding*, because this class is common for both user and item encoder.
- mentioned as *u* in the paper
"""

class UIEmbedding(nn.Module):
    def __init__(self,user_count,d_model):
        super().__init__()
        self.user_emb = nn.Embedding(user_count,d_model)
        self.ln = nn.LayerNorm(d_model)
        
    def forward(self,x):
        emb = self.user_emb(x)
        return self.ln(emb)

"""### User / Item Encoder
- Input: User/Item embedding, encoder output from User reviews /Item reviews
- Output: User / Item representation with respect to their reviews
- Refer section 4.2.3 in the paper
"""

class UIEncoder(nn.Module):
    def __init__(self,user_count,d_model,d_ff):
        super().__init__()
        self.attn = ScaledDotProductAttention(d_model)
        self.user_embedding = UIEmbedding(user_count,d_model)
        self.Wu = nn.Linear(d_model,d_model,bias=False)
        self.laye_norm = nn.LayerNorm(d_model)
        self.ffn = PositionwiseFeedForward(d_model,d_ff)
        
    def forward(self,x,enc_output,user_review_mask):
        emb = self.user_embedding(x)
        emb = self.Wu(emb)
        s_u,att = self.attn(emb,enc_output,enc_output,user_review_mask)
        h_u = self.laye_norm(emb + s_u)
        user_rep = self.ffn(h_u)
        return user_rep

user_count = len(user2idx)
user_count

config.hidden_size,config.intermediate_size

user_encoder = UIEncoder(user_count,config.hidden_size,config.intermediate_size)

def to_cuda(x):
    if torch.cuda.is_available():
        return x.cuda()
    return x

"""### Combine User encoder and reviews encoder"""

class PreTrainingModel(nn.Module):
    def __init__(self,bert_model,user_encoder):
        super().__init__()
        self.bert_model = bert_model
        self.user_encoder = user_encoder 
        
    def forward(self,input_ids,attention_mask,user_ids):
        seq_output, pooled_output = self.bert_model(input_ids=input_ids,attention_mask=attention_mask)
        user_rep = self.user_encoder(user_ids,seq_output,(attention_mask==0).unsqueeze(1))
        
        return user_rep

pretrain_model = PreTrainingModel(model,user_encoder)

"""### Final Layer 
- tranform user representation logits to 1-dimensional(rating score)
"""

class PreTrainStage(nn.Module):
    def __init__(self,pretrain_model,d_model):
        super().__init__()
        self.pretrain_model = pretrain_model
        self.out = nn.Linear(d_model,1)
        
    def forward(self,input_ids,attention_mask,user_ids):
        user_rep = self.pretrain_model(input_ids,attention_mask,user_ids)
        user_score = self.out(user_rep)
        return user_score

p_model =to_cuda(PreTrainStage(pretrain_model,config.hidden_size))

optimizer = optim.Adam(p_model.parameters(),lr=2e-5)

loss_fn = nn.MSELoss()

epochs=10

"""### Training Loop"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for epoch in tnrange(epochs):
#     dl = DataLoader(ds,batch_size=4,shuffle=True)
#     t = tqdm_notebook(dl,total=len(dl),leave=False)
#     trn_loss=[]
#     val_loss=[]
#     for data in t:
#         input_ids = to_cuda(data['input_ids'])
#         attention_mask = to_cuda(data['attention_mask'])
#         user_ids = to_cuda(data['user'])
#         targets = to_cuda(data['targets'])
#         
#         optimizer.zero_grad()
#         user_scores = p_model(input_ids,attention_mask,user_ids)
#         loss = loss_fn(user_scores.squeeze(),targets.squeeze())
#         loss.backward()
#         trn_loss.append(loss.item())
#         t.set_postfix(loss=loss.item())
#         optimizer.step()
#         
#     trn_loss = sum(trn_loss)/len(trn_loss)
#     
#     t = tqdm_notebook(val_dl,total=len(val_dl),leave=False)
#     with torch.no_grad():
#         for data in t:
#             input_ids = to_cuda(data['input_ids'])
#             attention_mask = to_cuda(data['attention_mask'])
#             user_ids = to_cuda(data['user'])
#             targets = to_cuda(data['targets'])
#             user_scores = p_model(input_ids,attention_mask,user_ids)
#             loss = loss_fn(user_scores.squeeze(),targets.squeeze())
#             val_loss.append(loss.item())
#             t.set_postfix(loss=loss.item())
#     val_loss = sum(val_loss)/len(val_loss)
#             
#     print("Epoch {} Train loss: {} Val loss: {}".format(epoch+1,round(trn_loss,5),round(val_loss,5)))

user_model_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/user_encoder.pt"
encoder_model_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/review_encoder.pt"

torch.save(user_encoder.state_dict(),user_model_fpath)

torch.save(model.state_dict(),encoder_model_fpath)

"""### Rating Prediction Model"""

TRAIN = False # during testing make it False

train = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/U BERT/train.csv")
max_len = 128
config = BertConfig()
user2idx_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/user2idx.pkl"
user2idx = joblib.load(user2idx_fpath)
user_count = len(user2idx)
PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
user_model_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/user_encoder.pt"
encoder_model_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/review_encoder.pt"

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

"""### Create Item to index"""

item2idx = {}
for i in train.asin.unique():
    item2idx[i]=len(item2idx)
len(item2idx)

item2idx = bidict(item2idx)

from torch.utils.data import Dataset,DataLoader

item2idx_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/item2idx.pkl"

if TRAIN:
    joblib.dump(item2idx,item2idx_fpath)

item2idx = joblib.load(item2idx_fpath)
item_count = len(item2idx)

item_count

"""### Create User,item pairs
- Model needs to be trained for this pairs
"""

user_item_pair=[]
for user,df in train[['reviewerID','asin','overall']].groupby('reviewerID'):
    for i,row in df.iterrows():
        user_item_pair.append((user,row.asin,row.overall))

len(user_item_pair)

user_item_pair[0]

"""### Dataset class for rating prediction model
- returns input_ids and attention_mask for each review
"""

class RPDataset(Dataset):
    def __init__(self,reviews,tokenizer,ui2idx,max_len):
        self.reviews = reviews
        self.max_len = max_len
        self.tokenizer = tokenizer
        self.ui2idx = ui2idx
    
    def __len__(self): return len(self.reviews)
    
    def __getitem__(self,item):
        
        ui_id = self.reviews[item][0]
        review_text = self.reviews[item][1]
        
        encoding = self.tokenizer.encode_plus(review_text,
                                              add_special_tokens=True,
                                              max_length=self.max_len,
                                              return_token_type_ids=False,
                                              pad_to_max_length=True,
                                              return_attention_mask=True,
                                              return_tensors='pt',
                                              truncation=True)
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }

"""### Combined dataset class for user and item reviews
- returns user_input_ids, user_attention_mask, item_input_ids, item_attention_mask, user_id, item_id, rating_score
"""

class UserDataset(Dataset):
    def __init__(self,user_item_pairs,tokenizer,max_len,user2idx,item2idx,review_ds,sample_len):
        self.user_item_pairs = user_item_pairs
        self.user2idx = user2idx
        self.item2idx = item2idx
        self.review_ds = review_ds
        self.sample_len = sample_len
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self): return len(self.user_item_pairs)
    
    def __getitem__(self,item):
        user,item,score = self.user_item_pairs[item]
        
        user_rev = self.review_ds[self.review_ds['reviewerID']==user]
        item_rev = self.review_ds[self.review_ds['asin']==item]
        
        if len(user_rev)>self.sample_len:
            user_rev = user_rev.sample(self.sample_len)
            
        if len(item_rev)>self.sample_len:
            item_rev = item_rev.sample(self.sample_len)
            
        user_ds = RPDataset(user_rev[['reviewerID','reviewText']].to_numpy(),self.tokenizer,self.user2idx,self.max_len)
        item_ds = RPDataset(item_rev[['asin','reviewText']].to_numpy(),self.tokenizer,self.item2idx,self.max_len)
        
        user_dl = next(iter(DataLoader(user_ds,batch_size=len(user_ds),shuffle=True)))
        item_dl = next(iter(DataLoader(item_ds,batch_size=len(item_ds),shuffle=True)))
        
        return {
            'user_dl' : user_dl,
            'item_dl' : item_dl,
            'user' : torch.LongTensor([self.user2idx[user]]),
            'item' : torch.LongTensor([self.item2idx[item]]),
            'score' : torch.Tensor([score])
        }

train= train[['reviewerID','asin','reviewText']]

user_ds = UserDataset(user_item_pair,tokenizer,max_len,user2idx,item2idx,train,10)

dl = DataLoader(user_ds,batch_size=1)

for data in dl:
    break

data['user_dl']

data['item_dl']

data['user']

data['item']

data['score']

"""### Function for attention mask
- this mask is used for co-matching layer
- masking padded token in item reviews / user reviews, when learning similarities between them.
- Refer co-matching layer
"""

def get_rev_attn_mask(att_mask,q_len):
    mask = att_mask.eq(0).repeat(q_len,1)
    return mask

"""### Co-Matching Layer
- exact implementation from paper
- Comprehensive user review representation(1 representation for all user reviews) from all user reviews with respect to item reviews is obtained.
- Comprehensive item review representation(1 representation for all item reviews) from all item reviews with respect to user reviews is obtained.
"""

class CoMatchingLayer(nn.Module):
    def __init__(self,d_model):
        super().__init__()
        self.u2i_attn = ScaledDotProductAttention(d_model)
        self.i2u_attn = ScaledDotProductAttention(d_model)
        
        self.WM = nn.Linear(2*d_model,d_model)
        
    def forward(self,user_review_enc,item_review_enc,user_mask,item_mask):
        Du,_ = self.u2i_attn(user_review_enc,item_review_enc,item_review_enc,user_mask)
        Di,_ = self.i2u_attn(item_review_enc,user_review_enc,user_review_enc,item_mask)
        
        Mu = F.tanh(self.WM(torch.cat([user_review_enc-Du,user_review_enc*Du],dim=-1)))
        Mi = F.tanh(self.WM(torch.cat([item_review_enc-Di,item_review_enc*Di],dim=-1)))
        
        tu,_ = Mu.max(0,keepdim=True)
        ti,_ = Mi.max(0,keepdim=True)
        
        return tu,ti

"""### Rating PRediction model
- user representation from user encoder
- item representation from item encoder
- user review representation from co-matching layer
- item review representation from co-matching layer

All of these 4 representation concatenated together and transformed to rating score.
"""

class RatingPrediction(nn.Module):
    def __init__(self,user_encoder,item_encoder,user_review_encoder,item_review_encoder,d_model):
        super().__init__()
        self.user_encoder= user_encoder
        self.item_encoder = item_encoder
        self.user_review_encoder = user_review_encoder
        self.item_review_encoder = item_review_encoder
        
        self.co_match_layer = CoMatchingLayer(d_model)
        self.d_model = d_model
        
        self.WR = nn.Linear(4*d_model,1)
        
    def forward(self,user_id,item_id,user_reviews,item_reviews,user_attn_mask,item_attn_mask):
        
        user_review_enc ,_= self.user_review_encoder(user_reviews,user_attn_mask)
        user_review_enc = user_review_enc.view(-1,self.d_model)
        
        user_enc_mask = (user_attn_mask.squeeze(0)==1).view(-1)
        user_rep = self.user_encoder(user_id,user_review_enc,user_enc_mask)
        
        item_review_enc,_ = self.item_review_encoder(item_reviews,item_attn_mask)
        item_review_enc = item_review_enc.view(-1,self.d_model)

        item_enc_mask = (item_attn_mask.squeeze(0)==1).view(-1)
        item_rep = self.item_encoder(item_id,item_review_enc,item_enc_mask)
        
        item_mask = get_rev_attn_mask(user_enc_mask,item_review_enc.shape[0])
        user_mask = get_rev_attn_mask(item_enc_mask,user_review_enc.shape[0])
        
        tu,ti = self.co_match_layer(user_review_enc,item_review_enc,user_mask,item_mask)
        
        rep = torch.cat([user_rep.squeeze(0),tu,item_rep.squeeze(0),ti],dim=-1)
        
        r = self.WR(rep)
        
        return r

user_encoder = UIEncoder(user_count,config.hidden_size,config.intermediate_size)

user_encoder.load_state_dict(torch.load(user_model_fpath))

user_review_encoder = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

user_review_encoder.load_state_dict(torch.load(encoder_model_fpath))

item_encoder = UIEncoder(item_count,config.hidden_size,config.intermediate_size)

item_review_encoder = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

item_review_encoder.load_state_dict(torch.load(encoder_model_fpath))

rp = to_cuda(RatingPrediction(user_encoder,item_encoder,user_review_encoder,item_review_encoder,config.hidden_size))

"""### Training loop"""

opt = optim.Adam(rp.parameters(),lr=1e-5)

epochs = 5

loss_fn = nn.MSELoss()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# for epoch in tnrange(epochs):
#     dl = DataLoader(user_ds,batch_size=1)
#     t = tqdm_notebook(dl,total=len(dl),leave=False)
#     trn_loss=[]
#     for data in t:
# 
#         user_ids = to_cuda(data['user_dl']['input_ids'].squeeze(0))
#         user_mask = to_cuda(data['user_dl']['attention_mask'].squeeze(0))
#         user = to_cuda(data['user'])
#         item_ids = to_cuda(data['item_dl']['input_ids'].squeeze(0))
#         item_mask = to_cuda(data['item_dl']['attention_mask'].squeeze(0))
#         item = to_cuda(data['item'])
#         target = to_cuda(data['score'])
#         
#         opt.zero_grad()
#         logits = rp(user,item,user_ids,item_ids,user_mask,item_mask)
#         loss = loss_fn(logits.squeeze(),target)
#         loss.backward()
#         opt.step()
#         trn_loss.append(loss.item())
#         t.set_postfix(loss=loss.item())
#     trn_loss = sum(trn_loss)/len(trn_loss)
#     print("Epoch: {} Train Loss: {}".format(epoch+1,trn_loss))
#



"""### Save & load model"""

rp_model_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/rp_model.pt"

if TRAIN:
    torch.save(rp.state_dict(),rp_model_fpath)

rp.load_state_dict(torch.load(rp_model_fpath))

"""### Test prediction"""

test = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/U BERT/test.csv")

"""### Create tokens_map
- testing is bit different from training dataset creation
- tokens map is dictionary which contains input_ids and attention_mask for all revoews
"""

tokens_map={}
for i,row in test.iterrows():
    encoding = tokenizer.encode_plus(row.reviewText,
                                      add_special_tokens=True,
                                      max_length=max_len,
                                      return_token_type_ids=False,
                                      pad_to_max_length=True,
                                      return_attention_mask=True,
                                      return_tensors='pt',
                                      truncation=True)
    tokens_map[i] = encoding

len(tokens_map)

"""### Create test map
- for (user, item) pair do
    - take all user reviews except his/her review for this item
    - take all item reviews(i.e, reviews from other users for this item), except user review for this item 

"""

test_map={}
for i,row in test.iterrows():
    users = test[test.reviewerID==row.reviewerID]
    items = test[test.asin==row.asin]
    if len(users)>=1 and len(items)>=1:
        user = [index for index in users.index if index!=i]
        item = [index for index in items.index if index!=i]
        if len(user)>=1 and len(item)>=1:
            test_map[(user2idx[row.reviewerID],item2idx[row.asin])]={
                'user':user,
                'item':item
            }

"""### TestDataset class
- returns user_input_ids, item_input_ids, user_id, item_id, user_attention_mask,item_attention_mask
"""

class TestDataset(Dataset):
    def __init__(self,test_map,tokens_map,sample_len):
        self.test_map = test_map
        self.tokens_map = tokens_map
        self.user_item_pair = list(test_map.keys())
        self.sample_len = sample_len
        
    def __len__(self): return len(self.test_map)
    
    def __getitem__(self,i):
        user,item = self.user_item_pair[i]
        user_rev = self.test_map[(user,item)]['user']
        if len(user_rev)>self.sample_len:
            user_rev = user_rev[:self.sample_len]
        item_rev = self.test_map[(user,item)] ['item']
        if len(item_rev)>self.sample_len:
            item_rev = item_rev[:self.sample_len]
        user_ids=[]
        user_att_mask=[]
        for idx in user_rev:
            user_ids.append(self.tokens_map[idx]['input_ids'].squeeze(0))
            user_att_mask.append(self.tokens_map[idx]['attention_mask'].squeeze(0))
        item_ids=[]
        item_att_mask=[]
        for idx in item_rev:
            item_ids.append(self.tokens_map[idx]['input_ids'].squeeze(0))
            item_att_mask.append(self.tokens_map[idx]['attention_mask'].squeeze(0))
            
        return {
            'user_ids':torch.stack(user_ids),
            'user_att_mask':torch.stack(user_att_mask),
            'item_ids':torch.stack(item_ids),
            'item_att_mask':torch.stack(item_att_mask),
            'user':torch.LongTensor([user]),
            'item':torch.LongTensor([item])
        }

test_ds = TestDataset(test_map,tokens_map,10)

test_dl = DataLoader(test_ds,batch_size=1)

"""### Prediction loop"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# t = tqdm_notebook(test_dl,total=len(test_dl),leave=True)
# output=[]
# with torch.no_grad():
#     for data in t:
#         user_ids = to_cuda(data['user_ids'].squeeze(0))
#         user_mask = to_cuda(data['user_att_mask'].squeeze(0))
#         user = to_cuda(data['user'])
#         item_ids = to_cuda(data['item_ids'].squeeze(0))
#         item_mask = to_cuda(data['item_att_mask'].squeeze(0))
#         item = to_cuda(data['item'])
# 
#         logits = rp(user,item,user_ids,item_ids,user_mask,item_mask)
#         output.append((user.item(),item.item(),logits.item()))
#

len(output)

output[1]

"""### Slice needed columns from test dataframe"""

test = test[['reviewerID','asin','reviewText','overall']]

"""### convert output for dataframe"""

predictions = pd.DataFrame(output,columns=['reviewerID','asin','PredictionScore'])
predictions.head()

"""### Convert user id, item id to actual user name, item name"""

user2idx = bidict(user2idx)

predictions.reviewerID = predictions.reviewerID.apply(lambda x: user2idx.inv[x])
predictions.asin = predictions.asin.apply(lambda x: item2idx.inv[x])

predictions.head(n=2)

"""### Merge 2 dataframe prediction and test
- to compare prediction score and actual score
"""

predictions.PredictionScore = predictions.PredictionScore.apply(lambda x: x if x<5 else 5)

predictions = pd.merge(test,predictions,on=['reviewerID','asin'])
predictions.head(n=2)

output_fpath = "/content/gdrive/My Drive/Colab Notebooks/U BERT/predictions.csv"

predictions.to_csv(output_fpath,index=False)

"""### Metrics"""

Y = predictions['overall']
Y_preds = predictions['PredictionScore']

"""##### RMSE


$RMSE = \large{\sqrt{\frac{\sum_{i=1}^{N}(Y-Ypreds)^2}{N}}}$
"""

rmse = np.sqrt(np.power(Y-Y_preds,2).sum()/len(Y))

rmse

"""## Custom input TEST"""

test = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/U BERT/test.csv")

user_id = "A00625243BI8W1SSZNLMD"
item_id = "B000SAC5PA"

## List of user reviews

user_reviews = [
    'My guitar, just Love it! This stand is exactly what I hoped it would be. It is all I want or need from a stand.',
     "I'm a new user, but seem to be a Great product!!!! Can't complaint!!! I finally got it and saw how great it is. It has plenty of roomy pockets, great padding, and is put together with great quality."
]

## List of item reviews

item_reviews = [
    'I was dissapointed to find out that you can only use one of theeffects at a time, good though they were. Too expensive foronly one effect at a time. Returned it.',
 'Works as advertised but the keys are stiff. Pads take too much presure to activate. Nice that I dont need another interface to connect it to my laptop.After the first few uses the USB socket broke and fell off into the unit rendering it useless. Wrote to dealer that was listed as selling it to me thru Amazon only to have them tell me the wrong seller was listed. I had registered it with AKAI already so I complained to them and they sent me a new one. I had to pay shipping back the old one to them even though it was a defective product.Not recommending this product.',
 'This was a great purchase.  My son found exactly what he needed and at a price he could afford.  He would definitely purchase from here again.'
]

user_rev=[]
user_attn=[]
for review_text in user_reviews:
    d = tokenizer.encode_plus(review_text,
                          add_special_tokens=True,
                          max_length=max_len,
                          return_token_type_ids=False,
                          pad_to_max_length=True,
                          return_attention_mask=True,
                          return_tensors='pt',
                          truncation=True)
    user_rev.append(d['input_ids'].squeeze())
    user_attn.append(d['attention_mask'].squeeze())

user_rev = to_cuda(torch.stack(user_rev,dim=0))
user_attn = to_cuda(torch.stack(user_attn,dim=0))

user_rev.shape,user_attn.shape

item_rev=[]
item_attn=[]
for review_text in item_reviews:
    d = tokenizer.encode_plus(review_text,
                          add_special_tokens=True,
                          max_length=max_len,
                          return_token_type_ids=False,
                          pad_to_max_length=True,
                          return_attention_mask=True,
                          return_tensors='pt',
                          truncation=True)
    item_rev.append(d['input_ids'].squeeze())
    item_attn.append(d['attention_mask'].squeeze())

item_rev = to_cuda(torch.stack(item_rev,dim=0))
item_attn = to_cuda(torch.stack(item_attn,dim=0))

item_rev.shape,item_attn.shape

user = to_cuda(torch.LongTensor([[user2idx[user_id]]]))

item = to_cuda(torch.LongTensor([[item2idx[item_id]]]))

with torch.no_grad():
    logits = rp(user,item,user_rev,item_rev,user_attn,item_attn)

score = logits.squeeze().item()

score

